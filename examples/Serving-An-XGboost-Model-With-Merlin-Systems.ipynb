{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cdba80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2022 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77acbcad",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "This notebook is created using the latest stable [merlin-tensorflow](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/merlin/containers/merlin-tensorflow/tags) container. This Jupyter notebook example demonstrates how to deploy an `XGBoost` model to Triton Inference Server (TIS) and generate prediction results for a given query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0b95d9",
   "metadata": {},
   "source": [
    "To begin, we download the `MovieLens 100k Dataset` and train an `XGBoost` model to predict a rating a user is likely to give to a movie.\n",
    "\n",
    "In this notebook we will focus on deploying our model and will breeze through data preprocessing and the training of the model. \n",
    "\n",
    "If you would like to learn more about training an `XGBoost` model using the Merlin Framework, please consult a tutorial available [here](https://github.com/NVIDIA-Merlin/models/blob/main/examples/07-Train-an-xgboost-model-using-the-Merlin-Models-API.ipynb).\n",
    "\n",
    "Let's begin by downloading the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efad6b8",
   "metadata": {},
   "source": [
    "# Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd0eae12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-02 09:50:39.952360: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:952] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-02 09:50:39.952868: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:952] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-02 09:50:39.953008: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:952] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "/usr/local/lib/python3.8/dist-packages/cudf/core/frame.py:384: UserWarning: The deep parameter is ignored and is only included for pandas compatibility.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from merlin.core.utils import Distributed\n",
    "from merlin.models.xgb import XGBoost\n",
    "import nvtabular as nvt\n",
    "import numpy as np\n",
    "\n",
    "from merlin.datasets.entertainment import get_movielens\n",
    "\n",
    "train, _ = get_movielens(variant='ml-100k')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238ea5dd",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed3334f",
   "metadata": {},
   "source": [
    "Let us now preprocess our data.\n",
    "\n",
    "We capture the preprocessing steps in a workflow and will be able to reuse them for preprocessing incoming requests during serving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89cf9f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_steps = ['movieId', 'userId', 'genres'] >> nvt.ops.Categorify(freq_threshold=2, dtype=np.int32)\n",
    "\n",
    "train_preprocessing_workflow = nvt.Workflow(preprocessing_steps + train.schema.remove_col('rating_binary').remove_col('title').column_names)\n",
    "train_transformed = train_preprocessing_workflow.fit_transform(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38e9037",
   "metadata": {},
   "source": [
    "# Train an XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d7f74cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-02 09:50:42,015 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize\n",
      "[09:50:43] task [xgboost.dask]:tcp://127.0.0.1:45235 got new rank 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:2.36952\n",
      "[20]\ttrain-rmse:0.95316\n",
      "[40]\ttrain-rmse:0.92447\n",
      "[60]\ttrain-rmse:0.90741\n",
      "[80]\ttrain-rmse:0.89437\n",
      "[84]\ttrain-rmse:0.89138\n"
     ]
    }
   ],
   "source": [
    "with Distributed():\n",
    "    model = XGBoost(schema=train_transformed.schema)\n",
    "    model.fit(\n",
    "        train_transformed,\n",
    "        num_boost_round=85,\n",
    "        verbose_eval=20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a3cf39",
   "metadata": {},
   "source": [
    "# Create the Ensemble Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc40083e",
   "metadata": {},
   "source": [
    "Let us now define an `Ensemble` that will be used for serving predictions on the Triton Inference Server.\n",
    "\n",
    "An `Ensemble` defines operations to be performed on incoming requests. It begins with specifying the input schema (fields that the inference request will contain).\n",
    "\n",
    "Our model was trained on data that included the `target` column. However, in production, this information will not be available to us. Let us modify the schema to reflect this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2728a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_schema = train_preprocessing_workflow.input_schema.remove_col('rating')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7c46d7",
   "metadata": {},
   "source": [
    "In general, you want to define a preprocessing workflow once and apply it throughout the lifecycle of your model, from training all the way to serving in production. Redifining the workflows on the go, or using custom written code for these operations, can be a source of subtle bugs.\n",
    "\n",
    "In order to ensure we process our data in the same way in production as we do in training, let us now modify the training preprocessing pipeline and use it to construct our inference workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa8dc34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_preprocessing_workflow = train_preprocessing_workflow.remove_inputs(['rating'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71c5636",
   "metadata": {},
   "source": [
    "Equipped with the modified data preprocessing workflow, let us define the full set of inference operations we will want to run on the Triton inference server.\n",
    "\n",
    "We begin by stating what data the server can expect (`inference_schema`). We proceed to wrap our `inference_preprocessing_workflow` in `TransformWorkflow` -- an operator we can leverage to executing our workflow during serving.\n",
    "\n",
    "Last but not least, having received and preprocessed the data, we instruct the Triton inference server to perform inference using the model that we trained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de9e2237",
   "metadata": {},
   "outputs": [],
   "source": [
    "from merlin.systems.dag.ops.fil import PredictForest\n",
    "from merlin.systems.dag.ensemble import Ensemble\n",
    "from merlin.systems.dag.ops.workflow import TransformWorkflow\n",
    "\n",
    "inference_ops = inference_schema.column_names >> TransformWorkflow(inference_preprocessing_workflow) \\\n",
    "                    >> PredictForest(model.booster, inference_preprocessing_workflow.output_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76dad9c3",
   "metadata": {},
   "source": [
    "With inference operations defined, all that remains now is outputting the ensemble to disk so that it can be loaded up when Triton starts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e23a7fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = Ensemble(inference_ops, inference_schema)\n",
    "ensemble.export('ensemble');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9165dfd",
   "metadata": {},
   "source": [
    "# Starting the Triton Inference Server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2feebda1",
   "metadata": {},
   "source": [
    "We now are ready to start the Triton Inference Server. We do so in a subprocess in order to continue our exploration in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e81d57b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<subprocess.Popen at 0x7fa1b031e8e0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0802 09:50:45.025498 1057 pinned_memory_manager.cc:240] Pinned memory pool is created at '0x7f05a6000000' with size 268435456\n",
      "I0802 09:50:45.025823 1057 cuda_memory_manager.cc:105] CUDA memory pool is created on device 0 with size 67108864\n",
      "I0802 09:50:45.027726 1057 model_repository_manager.cc:1191] loading: 1_fil:1\n",
      "I0802 09:50:45.127882 1057 model_repository_manager.cc:1191] loading: 0_transformworkflow:1\n",
      "I0802 09:50:45.137409 1057 initialize.hpp:43] TRITONBACKEND_Initialize: fil\n",
      "I0802 09:50:45.137421 1057 backend.hpp:47] Triton TRITONBACKEND API version: 1.9\n",
      "I0802 09:50:45.137424 1057 backend.hpp:52] 'fil' TRITONBACKEND API version: 1.9\n",
      "I0802 09:50:45.137694 1057 model_initialize.hpp:37] TRITONBACKEND_ModelInitialize: 1_fil (version 1)\n",
      "I0802 09:50:45.138704 1057 instance_initialize.hpp:46] TRITONBACKEND_ModelInstanceInitialize: 1_fil_0 (GPU device 0)\n",
      "I0802 09:50:45.156972 1057 model_repository_manager.cc:1345] successfully loaded '1_fil' version 1\n",
      "I0802 09:50:45.228012 1057 model_repository_manager.cc:1191] loading: 1_predictforest:1\n",
      "I0802 09:50:45.230221 1057 python.cc:2388] TRITONBACKEND_ModelInstanceInitialize: 0_transformworkflow (GPU device 0)\n",
      "I0802 09:50:46.585742 1057 model_repository_manager.cc:1345] successfully loaded '0_transformworkflow' version 1\n",
      "I0802 09:50:46.586228 1057 python.cc:2388] TRITONBACKEND_ModelInstanceInitialize: 1_predictforest (GPU device 0)\n",
      "I0802 09:50:48.093558 1057 model_repository_manager.cc:1345] successfully loaded '1_predictforest' version 1\n",
      "I0802 09:50:48.093800 1057 model_repository_manager.cc:1191] loading: ensemble_model:1\n",
      "I0802 09:50:48.194069 1057 model_repository_manager.cc:1345] successfully loaded 'ensemble_model' version 1\n",
      "I0802 09:50:48.194143 1057 server.cc:556] \n",
      "+------------------+------+\n",
      "| Repository Agent | Path |\n",
      "+------------------+------+\n",
      "+------------------+------+\n",
      "\n",
      "I0802 09:50:48.194185 1057 server.cc:583] \n",
      "+---------+-------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Backend | Path                                                  | Config                                                                                                                                                         |\n",
      "+---------+-------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| fil     | /opt/tritonserver/backends/fil/libtriton_fil.so       | {\"cmdline\":{\"auto-complete-config\":\"false\",\"min-compute-capability\":\"6.000000\",\"backend-directory\":\"/opt/tritonserver/backends\",\"default-max-batch-size\":\"4\"}} |\n",
      "| python  | /opt/tritonserver/backends/python/libtriton_python.so | {\"cmdline\":{\"auto-complete-config\":\"false\",\"min-compute-capability\":\"6.000000\",\"backend-directory\":\"/opt/tritonserver/backends\",\"default-max-batch-size\":\"4\"}} |\n",
      "+---------+-------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "I0802 09:50:48.194222 1057 server.cc:626] \n",
      "+---------------------+---------+--------+\n",
      "| Model               | Version | Status |\n",
      "+---------------------+---------+--------+\n",
      "| 0_transformworkflow | 1       | READY  |\n",
      "| 1_fil               | 1       | READY  |\n",
      "| 1_predictforest     | 1       | READY  |\n",
      "| ensemble_model      | 1       | READY  |\n",
      "+---------------------+---------+--------+\n",
      "\n",
      "I0802 09:50:48.217709 1057 metrics.cc:650] Collecting metrics for GPU 0: Quadro RTX 8000\n",
      "I0802 09:50:48.217961 1057 tritonserver.cc:2138] \n",
      "+----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Option                           | Value                                                                                                                                                                                        |\n",
      "+----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| server_id                        | triton                                                                                                                                                                                       |\n",
      "| server_version                   | 2.22.0                                                                                                                                                                                       |\n",
      "| server_extensions                | classification sequence model_repository model_repository(unload_dependents) schedule_policy model_configuration system_shared_memory cuda_shared_memory binary_tensor_data statistics trace |\n",
      "| model_repository_path[0]         | ensemble                                                                                                                                                                                     |\n",
      "| model_control_mode               | MODE_NONE                                                                                                                                                                                    |\n",
      "| strict_model_config              | 1                                                                                                                                                                                            |\n",
      "| rate_limit                       | OFF                                                                                                                                                                                          |\n",
      "| pinned_memory_pool_byte_size     | 268435456                                                                                                                                                                                    |\n",
      "| cuda_memory_pool_byte_size{0}    | 67108864                                                                                                                                                                                     |\n",
      "| response_cache_byte_size         | 0                                                                                                                                                                                            |\n",
      "| min_supported_compute_capability | 6.0                                                                                                                                                                                          |\n",
      "| strict_readiness                 | 1                                                                                                                                                                                            |\n",
      "| exit_timeout                     | 30                                                                                                                                                                                           |\n",
      "+----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "I0802 09:50:48.218561 1057 grpc_server.cc:4589] Started GRPCInferenceService at 0.0.0.0:8001\n",
      "I0802 09:50:48.218748 1057 http_server.cc:3303] Started HTTPService at 0.0.0.0:8000\n",
      "I0802 09:50:48.259455 1057 http_server.cc:178] Started Metrics Service at 0.0.0.0:8002\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "subprocess.Popen([\"tritonserver\", \"--model-repository=ensemble\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0a2d9c",
   "metadata": {},
   "source": [
    "The server is now running. We have pointed it to where our ensemble resides on disk via specifying the `--model-repository` parameter.\n",
    "\n",
    "Our inference pipeline has now been loaded onto the server and it is ready to receive inference requests.\n",
    "\n",
    "Let us issue a request and verify the results.\n",
    "\n",
    "We begin by obtaining 10 examples from our train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d61751b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>userId</th>\n",
       "      <th>genres</th>\n",
       "      <th>TE_movieId_rating</th>\n",
       "      <th>userId_count</th>\n",
       "      <th>gender</th>\n",
       "      <th>zip_code</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>77</td>\n",
       "      <td>43</td>\n",
       "      <td>0.779876</td>\n",
       "      <td>5.572154</td>\n",
       "      <td>1</td>\n",
       "      <td>77</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>231</td>\n",
       "      <td>77</td>\n",
       "      <td>13</td>\n",
       "      <td>-0.896619</td>\n",
       "      <td>5.572154</td>\n",
       "      <td>1</td>\n",
       "      <td>77</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>366</td>\n",
       "      <td>77</td>\n",
       "      <td>17</td>\n",
       "      <td>-0.954632</td>\n",
       "      <td>5.572154</td>\n",
       "      <td>1</td>\n",
       "      <td>77</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>96</td>\n",
       "      <td>77</td>\n",
       "      <td>89</td>\n",
       "      <td>-0.093809</td>\n",
       "      <td>5.572154</td>\n",
       "      <td>1</td>\n",
       "      <td>77</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>383</td>\n",
       "      <td>77</td>\n",
       "      <td>25</td>\n",
       "      <td>-0.539376</td>\n",
       "      <td>5.572154</td>\n",
       "      <td>1</td>\n",
       "      <td>77</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   movieId  userId  genres  TE_movieId_rating  userId_count  gender  zip_code  \\\n",
       "0        7      77      43           0.779876      5.572154       1        77   \n",
       "1      231      77      13          -0.896619      5.572154       1        77   \n",
       "2      366      77      17          -0.954632      5.572154       1        77   \n",
       "3       96      77      89          -0.093809      5.572154       1        77   \n",
       "4      383      77      25          -0.539376      5.572154       1        77   \n",
       "\n",
       "   age  \n",
       "0    1  \n",
       "1    1  \n",
       "2    1  \n",
       "3    1  \n",
       "4    1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ten_examples = train.compute().drop(columns=['rating', 'rating_binary', 'title'])[:10]\n",
    "ten_examples.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7808bc12",
   "metadata": {},
   "source": [
    "Now let's package the information up as inputs and send it to Triton for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2fefd5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from merlin.systems.triton import convert_df_to_triton_input\n",
    "import tritonclient.grpc as grpcclient\n",
    "\n",
    "ten_examples = train.compute().drop(columns=['rating', 'title', 'rating_binary'])[:10]\n",
    "inputs = convert_df_to_triton_input(inference_schema.column_names, ten_examples, grpcclient.InferInput)\n",
    "\n",
    "outputs = [\n",
    "    grpcclient.InferRequestedOutput(col)\n",
    "    for col in inference_ops.output_schema.column_names\n",
    "]\n",
    "# send request to tritonserver\n",
    "with grpcclient.InferenceServerClient(\"localhost:8001\") as client:\n",
    "    response = client.infer(\"ensemble_model\", inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc7909f",
   "metadata": {},
   "source": [
    "Let us now look at the results and compare to local predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ddd35cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_from_triton = response.as_numpy(outputs[0].name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f28fdfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/distributed/node.py:180: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 43463 instead\n",
      "  warnings.warn(\n",
      "2022-08-02 09:51:02,611 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize\n"
     ]
    }
   ],
   "source": [
    "with Distributed():\n",
    "    local_predictions = model.predict(train_transformed)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e946de27",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(predictions_from_triton, local_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8aa4456",
   "metadata": {},
   "source": [
    "We managed to preprocess the data in the same way in serving as we did during training and obtain the same predictions!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
