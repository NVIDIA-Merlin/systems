{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "620dd990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2022 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f550a0e5",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# Serving Ranking Models With Merlin Systems\n",
    "\n",
    "This notebook is created using the latest stable [merlin-tensorflow](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/merlin/containers/merlin-tensorflow/tags) container. This Jupyter notebook example demonstrates how to deploy a ranking model to Triton Inference Server (TIS) and generate prediction results for a given query. As a prerequisite, the ranking model must be trained and saved with Merlin Models. Please read the [README](https://github.com/NVIDIA-Merlin/systems/blob/main/examples/README.md) for the instructions.\n",
    "\n",
    "## Overview\n",
    "\n",
    "NVIDIA Merlin is an open source framework that accelerates and scales end-to-end recommender system pipelines. The Merlin framework is broken up into several sub components, these include: Merlin-Core, Merlin-Models, NVTabular and Merlin-Systems. Merlin Systems will be the focus of this example.\n",
    "\n",
    "The purpose of the Merlin Systems library is to make it easy for Merlin users to quickly deploy their recommender systems from development to [Triton Inference Server](https://github.com/triton-inference-server/server). We extended the same user-friendly API users are accustomed to in NVTabular and leveraged it to accommodate deploying recommender system components to TIS. \n",
    "\n",
    "There are some points we need ensure before we continue with this Notebook. Please ensure you have a working NVTabular workflow and model stored in an accessible location. Merlin Systems take the data preprocessing workflow defined in NVTabular and load that into Triton Inference Server as a model. Subsequently it does the same for the trained model. Lets take a closer look at how Merlin Systems makes deploying to TIS simple and effortless, in the rest of this notebook.\n",
    "\n",
    "### Learning objectives\n",
    "\n",
    "In this notebook, we learn how to deploy a NVTabular Workflow and a trained Tensorflow model from Merlin Models to Triton.\n",
    "- Load NVTabular Workflow\n",
    "- Load Pre-trained Merlin Models model\n",
    "- Create Ensemble Graph\n",
    "- Export Ensemble Graph\n",
    "- Run Tritonserver\n",
    "- Send Request to Tritonserver\n",
    "\n",
    "### Dataset\n",
    "\n",
    "We use the synthetic train and test datasets generated by mimicking the real [Ali-CCP: Alibaba Click and Conversion Prediction](https://tianchi.aliyun.com/dataset/dataDetail?dataId=408#1) dataset to build our recommender system ranking models. To see how the data is transformed with NVTabular and how a DLRM model is trained with Merlin Models check out the [04-Exporting-ranking-models.ipynb](https://github.com/NVIDIA-Merlin/models/blob/main/examples/04-Exporting-ranking-models.ipynb) example notebook which is a prerequisite for this notebook.\n",
    "\n",
    "It is important to note that the steps take in this notebook are generalized and can be applied to any set of workflow and models. \n",
    "\n",
    "### Tools\n",
    "\n",
    "- NVTabular\n",
    "- Merlin Models\n",
    "- Merlin Systems\n",
    "- Triton Inference Server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1346eec7-48d9-41b3-a3f7-c88230a6f24c",
   "metadata": {},
   "source": [
    "## Install Required Libraries\n",
    "\n",
    "Install TensorFlow so we can read the saved model from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a0b7e5d-197b-42ec-9213-5582a85d140e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow-gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222fde5f",
   "metadata": {},
   "source": [
    "## Load an NVTabular Workflow\n",
    "\n",
    "First, we load the `nvtabular.Workflow` that we created in with this [example](https://github.com/NVIDIA-Merlin/models/blob/main/examples/04-Exporting-ranking-models.ipynb). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1dc4a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_GPU_ALLOCATOR\"]=\"cuda_malloc_async\"\n",
    "from nvtabular.workflow import Workflow\n",
    "\n",
    "input_path = os.environ.get(\"INPUT_FOLDER\", \"/workspace/data/\")\n",
    "\n",
    "workflow_stored_path = os.path.join(input_path, \"workflow\")\n",
    "\n",
    "workflow = Workflow.load(workflow_stored_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746fba56-36af-44a3-8beb-34ca026648be",
   "metadata": {},
   "source": [
    "After we load the workflow, we remove the label columns from it's inputs. This removes all columns with the `TARGET` tag from the workflow. We do this because we need to set the workflow to only  require the features needed to predict, not train, when creating an inference pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcef202e-fa6f-4c23-aca6-0b965faf637e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<nvtabular.workflow.workflow.Workflow at 0x7ff5b42c9850>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from merlin.schema.tags import Tags\n",
    "\n",
    "label_columns = workflow.output_schema.select_by_tag(Tags.TARGET).column_names\n",
    "workflow.remove_inputs(label_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143795f2",
   "metadata": {},
   "source": [
    "## Load the Tensorflow Model\n",
    "\n",
    "After loading the workflow, we load the model. This model was trained with the output of the workflow from the [Exporting Ranking Models](https://github.com/NVIDIA-Merlin/models/blob/main/examples/04-Exporting-ranking-models.ipynb) example from Merlin Models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5b046f",
   "metadata": {},
   "source": [
    "First, we need to import the Merlin Models library. Loading a TensorFlow model, which is based on custom subclasses, requires to the subclass definition. Otherwise, TensorFlow cannot load correctly load the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffca456b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-12 17:18:23.722737: I tensorflow/core/platform/cpu_feature_guard.cc:152] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-07-12 17:18:25.872447: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:214] Using CUDA malloc Async allocator for GPU: 0\n",
      "2022-07-12 17:18:25.872791: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 16255 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB-LS, pci bus id: 0000:89:00.0, compute capability: 7.0\n"
     ]
    }
   ],
   "source": [
    "import merlin.models.tf as mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8da5e606",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-12 17:18:27.313679: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf_model_path = os.path.join(input_path, \"dlrm\")\n",
    "\n",
    "model = tf.keras.models.load_model(tf_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53908458",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create the Ensemble Graph\n",
    "\n",
    "After we have both the model and the workflow loaded, we can create the ensemble graph. You create the graph. The goal is to illustrate the path of data through your full system. In this example we only serve a workflow with a model, but you can add other components that help you meet your business logic requirements.\n",
    "\n",
    "Because this example has two components&mdash;a model and a workflow&mdash;we require two operators. These operators, also known as inference operators, are meant to abstract away all the \"hard parts\" of loading a specific component, such as a workflow or model, into Triton Inference Server. \n",
    "\n",
    "The following code block shows how to use two inference operators:\n",
    "\n",
    "<dl>\n",
    "    <dt><code>TransformWorkflow</code></dt>\n",
    "    <dd>This operator ensures that the workflow is correctly saved and packaged with the required config so the server will know how to load it.</dd>\n",
    "    <dt><code>PredictTensorflow</code></dt>\n",
    "    <dd>This operator will do something similar with the model, loaded before.</dd>\n",
    "</dl>\n",
    "\n",
    "Let's give it a try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f80e5cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from merlin.systems.dag.ops.workflow import TransformWorkflow\n",
    "from merlin.systems.dag.ops.tensorflow import PredictTensorflow\n",
    "\n",
    "serving_operators = workflow.input_schema.column_names >> TransformWorkflow(workflow) >> PredictTensorflow(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b916afa",
   "metadata": {},
   "source": [
    "## Export Graph as Ensemble\n",
    "\n",
    "The last step is to create the ensemble artifacts that Triton Inference Server can consume.\n",
    "To make these artifacts, we import the `Ensemble` class.\n",
    "The class is responsible for interpreting the graph and exporting the correct files for the server.\n",
    "\n",
    "After you run the following cell, you'll see that we create a `ColumnSchema` for the expected inputs to the workflow.\n",
    "The workflow is a `Schema`. \n",
    "\n",
    "When you are creating an `Ensemble` object you supply the graph and a schema representing the starting input of the graph. the inputs to the ensemble graph are the inputs to the first operator of your graph. \n",
    "\n",
    "After you have created the `Ensemble` you export the graph, supplying an export path for the `Ensemble.export` function.\n",
    "\n",
    "This returns an ensemble config which represents the entire inference pipeline and a list of node-specific configs.\n",
    "\n",
    "Let's take a look below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c9a7cd2-e14e-4b37-b0af-3dc3931c9ff9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>tags</th>\n",
       "      <th>dtype</th>\n",
       "      <th>is_list</th>\n",
       "      <th>is_ragged</th>\n",
       "      <th>properties.num_buckets</th>\n",
       "      <th>properties.freq_threshold</th>\n",
       "      <th>properties.max_size</th>\n",
       "      <th>properties.start_index</th>\n",
       "      <th>properties.cat_path</th>\n",
       "      <th>properties.domain.min</th>\n",
       "      <th>properties.domain.max</th>\n",
       "      <th>properties.domain.name</th>\n",
       "      <th>properties.embedding_sizes.cardinality</th>\n",
       "      <th>properties.embedding_sizes.dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>user_id</td>\n",
       "      <td>(Tags.USER, Tags.USER_ID, Tags.CATEGORICAL)</td>\n",
       "      <td>int64</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>.//categories/unique.user_id.parquet</td>\n",
       "      <td>0</td>\n",
       "      <td>755</td>\n",
       "      <td>user_id</td>\n",
       "      <td>755</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>item_id</td>\n",
       "      <td>(Tags.ITEM_ID, Tags.ITEM, Tags.CATEGORICAL)</td>\n",
       "      <td>int64</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>.//categories/unique.item_id.parquet</td>\n",
       "      <td>0</td>\n",
       "      <td>772</td>\n",
       "      <td>item_id</td>\n",
       "      <td>772</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>item_category</td>\n",
       "      <td>(Tags.ITEM, Tags.CATEGORICAL)</td>\n",
       "      <td>int64</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>.//categories/unique.item_category.parquet</td>\n",
       "      <td>0</td>\n",
       "      <td>772</td>\n",
       "      <td>item_category</td>\n",
       "      <td>772</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>item_shop</td>\n",
       "      <td>(Tags.ITEM, Tags.CATEGORICAL)</td>\n",
       "      <td>int64</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>.//categories/unique.item_shop.parquet</td>\n",
       "      <td>0</td>\n",
       "      <td>772</td>\n",
       "      <td>item_shop</td>\n",
       "      <td>772</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>item_brand</td>\n",
       "      <td>(Tags.ITEM, Tags.CATEGORICAL)</td>\n",
       "      <td>int64</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>.//categories/unique.item_brand.parquet</td>\n",
       "      <td>0</td>\n",
       "      <td>772</td>\n",
       "      <td>item_brand</td>\n",
       "      <td>772</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>user_shops</td>\n",
       "      <td>(Tags.USER, Tags.CATEGORICAL)</td>\n",
       "      <td>int64</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>.//categories/unique.user_shops.parquet</td>\n",
       "      <td>0</td>\n",
       "      <td>755</td>\n",
       "      <td>user_shops</td>\n",
       "      <td>755</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>user_profile</td>\n",
       "      <td>(Tags.USER, Tags.CATEGORICAL)</td>\n",
       "      <td>int64</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>.//categories/unique.user_profile.parquet</td>\n",
       "      <td>0</td>\n",
       "      <td>67</td>\n",
       "      <td>user_profile</td>\n",
       "      <td>67</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>user_group</td>\n",
       "      <td>(Tags.USER, Tags.CATEGORICAL)</td>\n",
       "      <td>int64</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>.//categories/unique.user_group.parquet</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>user_group</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>user_gender</td>\n",
       "      <td>(Tags.USER, Tags.CATEGORICAL)</td>\n",
       "      <td>int64</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>.//categories/unique.user_gender.parquet</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>user_gender</td>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>user_age</td>\n",
       "      <td>(Tags.USER, Tags.CATEGORICAL)</td>\n",
       "      <td>int64</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>.//categories/unique.user_age.parquet</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>user_age</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>user_consumption_2</td>\n",
       "      <td>(Tags.USER, Tags.CATEGORICAL)</td>\n",
       "      <td>int64</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>.//categories/unique.user_consumption_2.parquet</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>user_consumption_2</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>user_is_occupied</td>\n",
       "      <td>(Tags.USER, Tags.CATEGORICAL)</td>\n",
       "      <td>int64</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>.//categories/unique.user_is_occupied.parquet</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>user_is_occupied</td>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>user_geography</td>\n",
       "      <td>(Tags.USER, Tags.CATEGORICAL)</td>\n",
       "      <td>int64</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>.//categories/unique.user_geography.parquet</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>user_geography</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>user_intentions</td>\n",
       "      <td>(Tags.USER, Tags.CATEGORICAL)</td>\n",
       "      <td>int64</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>.//categories/unique.user_intentions.parquet</td>\n",
       "      <td>0</td>\n",
       "      <td>755</td>\n",
       "      <td>user_intentions</td>\n",
       "      <td>755</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>user_brands</td>\n",
       "      <td>(Tags.USER, Tags.CATEGORICAL)</td>\n",
       "      <td>int64</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>.//categories/unique.user_brands.parquet</td>\n",
       "      <td>0</td>\n",
       "      <td>755</td>\n",
       "      <td>user_brands</td>\n",
       "      <td>755</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>user_categories</td>\n",
       "      <td>(Tags.USER, Tags.CATEGORICAL)</td>\n",
       "      <td>int64</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>.//categories/unique.user_categories.parquet</td>\n",
       "      <td>0</td>\n",
       "      <td>755</td>\n",
       "      <td>user_categories</td>\n",
       "      <td>755</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "[{'name': 'user_id', 'tags': {<Tags.USER: 'user'>, <Tags.USER_ID: 'user_id'>, <Tags.CATEGORICAL: 'categorical'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0, 'max_size': 0, 'start_index': 0, 'cat_path': './/categories/unique.user_id.parquet', 'domain': {'min': 0, 'max': 755, 'name': 'user_id'}, 'embedding_sizes': {'cardinality': 755, 'dimension': 65}}, 'dtype': dtype('int64'), 'is_list': False, 'is_ragged': False}, {'name': 'item_id', 'tags': {<Tags.ITEM_ID: 'item_id'>, <Tags.ITEM: 'item'>, <Tags.CATEGORICAL: 'categorical'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0, 'max_size': 0, 'start_index': 0, 'cat_path': './/categories/unique.item_id.parquet', 'domain': {'min': 0, 'max': 772, 'name': 'item_id'}, 'embedding_sizes': {'cardinality': 772, 'dimension': 66}}, 'dtype': dtype('int64'), 'is_list': False, 'is_ragged': False}, {'name': 'item_category', 'tags': {<Tags.ITEM: 'item'>, <Tags.CATEGORICAL: 'categorical'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0, 'max_size': 0, 'start_index': 0, 'cat_path': './/categories/unique.item_category.parquet', 'domain': {'min': 0, 'max': 772, 'name': 'item_category'}, 'embedding_sizes': {'cardinality': 772, 'dimension': 66}}, 'dtype': dtype('int64'), 'is_list': False, 'is_ragged': False}, {'name': 'item_shop', 'tags': {<Tags.ITEM: 'item'>, <Tags.CATEGORICAL: 'categorical'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0, 'max_size': 0, 'start_index': 0, 'cat_path': './/categories/unique.item_shop.parquet', 'domain': {'min': 0, 'max': 772, 'name': 'item_shop'}, 'embedding_sizes': {'cardinality': 772, 'dimension': 66}}, 'dtype': dtype('int64'), 'is_list': False, 'is_ragged': False}, {'name': 'item_brand', 'tags': {<Tags.ITEM: 'item'>, <Tags.CATEGORICAL: 'categorical'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0, 'max_size': 0, 'start_index': 0, 'cat_path': './/categories/unique.item_brand.parquet', 'domain': {'min': 0, 'max': 772, 'name': 'item_brand'}, 'embedding_sizes': {'cardinality': 772, 'dimension': 66}}, 'dtype': dtype('int64'), 'is_list': False, 'is_ragged': False}, {'name': 'user_shops', 'tags': {<Tags.USER: 'user'>, <Tags.CATEGORICAL: 'categorical'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0, 'max_size': 0, 'start_index': 0, 'cat_path': './/categories/unique.user_shops.parquet', 'domain': {'min': 0, 'max': 755, 'name': 'user_shops'}, 'embedding_sizes': {'cardinality': 755, 'dimension': 65}}, 'dtype': dtype('int64'), 'is_list': False, 'is_ragged': False}, {'name': 'user_profile', 'tags': {<Tags.USER: 'user'>, <Tags.CATEGORICAL: 'categorical'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0, 'max_size': 0, 'start_index': 0, 'cat_path': './/categories/unique.user_profile.parquet', 'domain': {'min': 0, 'max': 67, 'name': 'user_profile'}, 'embedding_sizes': {'cardinality': 67, 'dimension': 17}}, 'dtype': dtype('int64'), 'is_list': False, 'is_ragged': False}, {'name': 'user_group', 'tags': {<Tags.USER: 'user'>, <Tags.CATEGORICAL: 'categorical'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0, 'max_size': 0, 'start_index': 0, 'cat_path': './/categories/unique.user_group.parquet', 'domain': {'min': 0, 'max': 13, 'name': 'user_group'}, 'embedding_sizes': {'cardinality': 13, 'dimension': 16}}, 'dtype': dtype('int64'), 'is_list': False, 'is_ragged': False}, {'name': 'user_gender', 'tags': {<Tags.USER: 'user'>, <Tags.CATEGORICAL: 'categorical'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0, 'max_size': 0, 'start_index': 0, 'cat_path': './/categories/unique.user_gender.parquet', 'domain': {'min': 0, 'max': 3, 'name': 'user_gender'}, 'embedding_sizes': {'cardinality': 3, 'dimension': 16}}, 'dtype': dtype('int64'), 'is_list': False, 'is_ragged': False}, {'name': 'user_age', 'tags': {<Tags.USER: 'user'>, <Tags.CATEGORICAL: 'categorical'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0, 'max_size': 0, 'start_index': 0, 'cat_path': './/categories/unique.user_age.parquet', 'domain': {'min': 0, 'max': 8, 'name': 'user_age'}, 'embedding_sizes': {'cardinality': 8, 'dimension': 16}}, 'dtype': dtype('int64'), 'is_list': False, 'is_ragged': False}, {'name': 'user_consumption_2', 'tags': {<Tags.USER: 'user'>, <Tags.CATEGORICAL: 'categorical'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0, 'max_size': 0, 'start_index': 0, 'cat_path': './/categories/unique.user_consumption_2.parquet', 'domain': {'min': 0, 'max': 4, 'name': 'user_consumption_2'}, 'embedding_sizes': {'cardinality': 4, 'dimension': 16}}, 'dtype': dtype('int64'), 'is_list': False, 'is_ragged': False}, {'name': 'user_is_occupied', 'tags': {<Tags.USER: 'user'>, <Tags.CATEGORICAL: 'categorical'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0, 'max_size': 0, 'start_index': 0, 'cat_path': './/categories/unique.user_is_occupied.parquet', 'domain': {'min': 0, 'max': 3, 'name': 'user_is_occupied'}, 'embedding_sizes': {'cardinality': 3, 'dimension': 16}}, 'dtype': dtype('int64'), 'is_list': False, 'is_ragged': False}, {'name': 'user_geography', 'tags': {<Tags.USER: 'user'>, <Tags.CATEGORICAL: 'categorical'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0, 'max_size': 0, 'start_index': 0, 'cat_path': './/categories/unique.user_geography.parquet', 'domain': {'min': 0, 'max': 5, 'name': 'user_geography'}, 'embedding_sizes': {'cardinality': 5, 'dimension': 16}}, 'dtype': dtype('int64'), 'is_list': False, 'is_ragged': False}, {'name': 'user_intentions', 'tags': {<Tags.USER: 'user'>, <Tags.CATEGORICAL: 'categorical'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0, 'max_size': 0, 'start_index': 0, 'cat_path': './/categories/unique.user_intentions.parquet', 'domain': {'min': 0, 'max': 755, 'name': 'user_intentions'}, 'embedding_sizes': {'cardinality': 755, 'dimension': 65}}, 'dtype': dtype('int64'), 'is_list': False, 'is_ragged': False}, {'name': 'user_brands', 'tags': {<Tags.USER: 'user'>, <Tags.CATEGORICAL: 'categorical'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0, 'max_size': 0, 'start_index': 0, 'cat_path': './/categories/unique.user_brands.parquet', 'domain': {'min': 0, 'max': 755, 'name': 'user_brands'}, 'embedding_sizes': {'cardinality': 755, 'dimension': 65}}, 'dtype': dtype('int64'), 'is_list': False, 'is_ragged': False}, {'name': 'user_categories', 'tags': {<Tags.USER: 'user'>, <Tags.CATEGORICAL: 'categorical'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0, 'max_size': 0, 'start_index': 0, 'cat_path': './/categories/unique.user_categories.parquet', 'domain': {'min': 0, 'max': 755, 'name': 'user_categories'}, 'embedding_sizes': {'cardinality': 755, 'dimension': 65}}, 'dtype': dtype('int64'), 'is_list': False, 'is_ragged': False}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow.output_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c102204b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from merlin.systems.dag.ensemble import Ensemble\n",
    "import numpy as np\n",
    "\n",
    "ensemble = Ensemble(serving_operators, workflow.input_schema)\n",
    "\n",
    "export_path = os.path.join(input_path, \"ensemble\")\n",
    "\n",
    "ens_conf, node_confs = ensemble.export(export_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edcbd33-f845-4921-b295-46ad366da722",
   "metadata": {},
   "source": [
    "Display the path to the directory with the ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64cc2ad6-78c7-47cf-b6bb-2de84a09f97e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/models/examples/ensemble'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "export_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96aa55fb",
   "metadata": {},
   "source": [
    "## Verification of Ensemble Artifacts\n",
    "\n",
    "After we export the ensemble, we can check the export path for the graph's artifacts. The directory structure represents an ordering number followed by an operator identifier such as `1_transformworkflow`, `2_predicttensorflow`, and so on.\n",
    "\n",
    "Inside each of those directories, the `export` method writes a `config.pbtxt` file and a directory with a number. The number indicates the version and begins at 1. The artifacts for each operator are found inside the `version` folder. These artifacts vary depending on the operator in use. \n",
    "\n",
    "Install the `seedir` python package so we can view some of the directory contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b48b3e0-ca84-4e63-90d3-f507f4f99ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seedir in /usr/local/lib/python3.8/dist-packages (0.3.1)\r\n",
      "Requirement already satisfied: natsort in /usr/local/lib/python3.8/dist-packages (from seedir) (8.1.0)\r\n",
      "Requirement already satisfied: emoji in /usr/local/lib/python3.8/dist-packages (from seedir) (1.7.0)\r\n"
     ]
    }
   ],
   "source": [
    "# install seedir\n",
    "!pip install seedir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce10c04f-e3a4-4886-a5de-644dfe41c6e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ensemble/\n",
      "├─0_transformworkflow/\n",
      "│ ├─1/\n",
      "│ │ ├─__pycache__/\n",
      "│ │ ├─model.py\n",
      "│ │ └─workflow/\n",
      "│ └─config.pbtxt\n",
      "├─1_predicttensorflow/\n",
      "│ ├─1/\n",
      "│ │ └─model.savedmodel/\n",
      "│ └─config.pbtxt\n",
      "└─ensemble_model/\n",
      "  ├─1/\n",
      "  └─config.pbtxt\n"
     ]
    }
   ],
   "source": [
    "import seedir as sd\n",
    "\n",
    "sd.seedir(export_path, style='lines', itemlimit=10, depthlimit=3, exclude_folders='.ipynb_checkpoints', sort=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b99146c",
   "metadata": {},
   "source": [
    "## Starting Triton Inference Server\n",
    "\n",
    "After we export the ensemble, we are ready to start the Triton Inference Server. The server is installed in all the Merlin inference containers.  If you are not using one of our containers, then ensure it is installed in your environment. For more information, see the Triton Inference Server [documentation](https://github.com/triton-inference-server/server/blob/r22.03/README.md#documentation). \n",
    "\n",
    "You can start the server by running the following command:\n",
    "\n",
    "```shell\n",
    "tritonserver --model-repository=/workspace/data/ensemble --backend-config=tensorflow,version=2\n",
    "```\n",
    "\n",
    "For the `--model-repository` argument, specify the same value as the `export_path` that you specified previously in the `ensemble.export` method.\n",
    "\n",
    "After you run the `tritonserver` command, wait until your terminal shows messages like the following example:\n",
    "\n",
    "```shell\n",
    "I0414 18:29:50.741833 4067 grpc_server.cc:4421] Started GRPCInferenceService at 0.0.0.0:8001\n",
    "I0414 18:29:50.742197 4067 http_server.cc:3113] Started HTTPService at 0.0.0.0:8000\n",
    "I0414 18:29:50.783470 4067 http_server.cc:178] Started Metrics Service at 0.0.0.0:8002\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75e2eb9",
   "metadata": {},
   "source": [
    "## Retrieving Recommendations from Triton Inference Server\n",
    "\n",
    "Now that our server is running, we can send requests to it. This request is composed of values that correspond to the request schema that was created when we exported the ensemble graph.\n",
    "\n",
    "In the code below we create a request to send to triton and send it. We will then analyze the response, to show the full experience.\n",
    "\n",
    "First we need to ensure that we have a client connected to the server that we started. To do this, we use the Triton HTTP client library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21a693ab-0ee3-443e-8ad4-fad030ec1985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client created.\n"
     ]
    }
   ],
   "source": [
    "import tritonclient.http as client\n",
    "\n",
    "# Create a triton client\n",
    "try:\n",
    "    triton_client = client.InferenceServerClient(url=\"localhost:8000\", verbose=True)\n",
    "    print(\"client created.\")\n",
    "except Exception as e:\n",
    "    print(\"channel creation failed: \" + str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358bb59b-796a-4e1e-8e02-b7d3b86c27f9",
   "metadata": {},
   "source": [
    "After we create the client and verified it is connected to the server instance, we can communicate with the server and ensure all the models are loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae48a6ee-d585-421b-bb5a-1a5c2edca058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET /v2/health/live, headers None\n",
      "<HTTPSocketPoolResponse status=200 headers={'content-length': '0', 'content-type': 'text/plain'}>\n",
      "POST /v2/repository/index, headers None\n",
      "\n",
      "<HTTPSocketPoolResponse status=200 headers={'content-type': 'application/json', 'content-length': '179'}>\n",
      "bytearray(b'[{\"name\":\"0_transformworkflow\",\"version\":\"1\",\"state\":\"READY\"},{\"name\":\"1_predicttensorflow\",\"version\":\"1\",\"state\":\"READY\"},{\"name\":\"ensemble_model\",\"version\":\"1\",\"state\":\"READY\"}]')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'name': '0_transformworkflow', 'version': '1', 'state': 'READY'},\n",
       " {'name': '1_predicttensorflow', 'version': '1', 'state': 'READY'},\n",
       " {'name': 'ensemble_model', 'version': '1', 'state': 'READY'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ensure triton is in a good state\n",
    "triton_client.is_server_live()\n",
    "triton_client.get_model_repository_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbc6f01-5ccf-43a2-ae4f-8451a420fc70",
   "metadata": {},
   "source": [
    "After verifying the models are correctly loaded by the server, we use some original validation data and send it as an inference request to the server. Here the `valid` folder was generated after `04-Exporting-ranking-models.ipynb` notebook is run. \n",
    "\n",
    "> The `df_lib` object is `cudf` if a GPU is available and `pandas` otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3235eb79-494a-4d92-b3d7-b74091c4e28b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>item_category</th>\n",
       "      <th>item_shop</th>\n",
       "      <th>item_brand</th>\n",
       "      <th>user_shops</th>\n",
       "      <th>user_profile</th>\n",
       "      <th>user_group</th>\n",
       "      <th>user_gender</th>\n",
       "      <th>user_age</th>\n",
       "      <th>user_consumption_2</th>\n",
       "      <th>user_is_occupied</th>\n",
       "      <th>user_geography</th>\n",
       "      <th>user_intentions</th>\n",
       "      <th>user_brands</th>\n",
       "      <th>user_categories</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>__null_dask_index__</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>700000</th>\n",
       "      <td>55</td>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "      <td>1305</td>\n",
       "      <td>450</td>\n",
       "      <td>2981</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>863</td>\n",
       "      <td>1482</td>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700001</th>\n",
       "      <td>53</td>\n",
       "      <td>8</td>\n",
       "      <td>33</td>\n",
       "      <td>2283</td>\n",
       "      <td>787</td>\n",
       "      <td>2871</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>831</td>\n",
       "      <td>1427</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700002</th>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>24</td>\n",
       "      <td>1631</td>\n",
       "      <td>562</td>\n",
       "      <td>497</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>144</td>\n",
       "      <td>247</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     user_id  item_id  item_category  item_shop  item_brand  \\\n",
       "__null_dask_index__                                                           \n",
       "700000                    55        5             19       1305         450   \n",
       "700001                    53        8             33       2283         787   \n",
       "700002                    10        6             24       1631         562   \n",
       "\n",
       "                     user_shops  user_profile  user_group  user_gender  \\\n",
       "__null_dask_index__                                                      \n",
       "700000                     2981             3           1            1   \n",
       "700001                     2871             3           1            1   \n",
       "700002                      497             1           1            1   \n",
       "\n",
       "                     user_age  user_consumption_2  user_is_occupied  \\\n",
       "__null_dask_index__                                                   \n",
       "700000                      1                   1                 1   \n",
       "700001                      1                   1                 1   \n",
       "700002                      1                   1                 1   \n",
       "\n",
       "                     user_geography  user_intentions  user_brands  \\\n",
       "__null_dask_index__                                                 \n",
       "700000                            1              863         1482   \n",
       "700001                            1              831         1427   \n",
       "700002                            1              144          247   \n",
       "\n",
       "                     user_categories  \n",
       "__null_dask_index__                   \n",
       "700000                           156  \n",
       "700001                           150  \n",
       "700002                            26  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from merlin.core.dispatch import get_lib\n",
    "\n",
    "df_lib = get_lib()\n",
    "\n",
    "original_data_path = os.environ.get(\"INPUT_FOLDER\", \"/workspace/data/\")\n",
    "\n",
    "# read in data for request\n",
    "batch = df_lib.read_parquet(\n",
    "    os.path.join(original_data_path,\"valid\", \"part.0.parquet\"), num_rows=3, columns=workflow.input_schema.column_names\n",
    ")\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9691d28f-80d8-4520-b6e6-b7e260c50380",
   "metadata": {},
   "source": [
    "After we isolate our `batch`, we convert the dataframe representation into inputs for Triton. We also declare the outputs that we expect to receive from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe53865c-9099-478c-8a30-17140ec1c120",
   "metadata": {},
   "outputs": [],
   "source": [
    "from merlin.systems.triton import convert_df_to_triton_input\n",
    "import tritonclient.grpc as grpcclient\n",
    "# create inputs and outputs\n",
    "\n",
    "inputs = convert_df_to_triton_input(workflow.input_schema.column_names, batch, grpcclient.InferInput)\n",
    "\n",
    "outputs = [\n",
    "    grpcclient.InferRequestedOutput(col)\n",
    "    for col in ensemble.graph.output_schema.column_names\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fb442d-d918-4b1a-a825-dfce4f75bc02",
   "metadata": {},
   "source": [
    "Now that our `inputs` and `outputs` are created, we can use the `triton_client` that we created earlier to send the inference request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c9031d7-a242-4278-808f-26d307e6a0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# send request to tritonserver\n",
    "with grpcclient.InferenceServerClient(\"localhost:8001\") as client:\n",
    "    response = client.infer(\"ensemble_model\", inputs, request_id=\"1\", outputs=outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c00a98-a30e-45ca-9ab6-667621a20c75",
   "metadata": {},
   "source": [
    "When the server completes the inference request, it returns a response. This response is parsed to get the desired predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4700c280-73e9-400a-ae9a-ee723789c96d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "click/binary_classification_task [[0.49801633]\n",
      " [0.49624982]\n",
      " [0.49931964]] (3, 1)\n"
     ]
    }
   ],
   "source": [
    "# access individual response columns to get values back.\n",
    "for col in ensemble.graph.output_schema.column_names:\n",
    "    print(col, response.as_numpy(col), response.as_numpy(col).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37053aa-9a01-40ce-ac81-e6edf266f2af",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This sample notebook started with an exported DLRM model and workflow.  We saw how to create an ensemble graph, verify the ensemble artifacts in the file system, and then put the ensemble into production with Triton Inference Server.  Finally, we sent a simple inference request to the server and printed the response."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "merlin": {
   "containers": [
    "nvcr.io/nvidia/merlin/merlin-tensorflow:latest"
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
