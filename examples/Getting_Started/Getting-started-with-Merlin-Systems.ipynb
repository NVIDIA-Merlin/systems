{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "620dd990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2022 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f550a0e5",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# Getting Started with Merlin Systems \n",
    "\n",
    "## Overview\n",
    "\n",
    "NVIDIA Merlin is an open source framework that accelerates and scales end-to-end recommender system pipelines on GPU. The Merlin framework is broken up into several sub components, these include: merlin-core, merlin-models, nvtabular and merlin-systems. Merlin Systems will be the focus of this example.\n",
    "\n",
    "The purpose of the Merlin Systems library is to make it easy for merlin users to quickly deploy their recommender systems from development to triton. We extended the same user-friendly API users are accustomed to in NVTabular and leveraged it to accommodate deploying your recommender system components to triton. \n",
    "\n",
    "There are some things we need ensure before we continue with this Notebook. Please ensure you have a working workflow and model stored in an accessible location. As previously mentioned, Merlin Systems will take the data preprocessing workflow defined in nvtabular and load that into triton as a model. Subsequently it will do the same for the trained model. Lets take a closer look in the rest of this notebook at how merlin systems makes deploying to tritonserver simple and effortless.\n",
    "\n",
    "\n",
    "**Be sure to check the other components of the Merlin framework, they can help you **\n",
    "\n",
    "### Learning objectives\n",
    "\n",
    "In this notebook, we learn how to deploy a NVTabular Workflow and a trained model from Merlin Models to tritonserver.\n",
    "- Load NVtabular Workflow\n",
    "- Load Pre-trained Merlin Models model\n",
    "- Create Ensemble Graph\n",
    "- Export Ensemble Graph\n",
    "- Run Tritonserver\n",
    "- Send Request to Tritonserver\n",
    "\n",
    "### Dataset\n",
    "\n",
    "In this notebook, we will be leveraging the Alibaba dataset[add link here]. It is important to note that the steps will take in this notebook are generalized and can be applied to any set of workflow and models. To see how the data is transformed please check the NVTabular [lin here nvtabular example] example for the Alibaba dataset. And to see how an Alibaba dataset trained model is created check the merlin-models project[link here merlin-models]\n",
    "\n",
    "### Tools\n",
    "\n",
    "- NVTabular\n",
    "- Merlin Models\n",
    "- Merlin Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222fde5f",
   "metadata": {},
   "source": [
    "## Load Workflow\n",
    "\n",
    "First, we will load the workflow created in with this example [link to nvtabular workflow example for alibaba dataset]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b16c0314-59a2-482a-bc0a-2328c4ed5a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nvtabular\n",
    "input_path = \"/merlin-models-data/movielens/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1dc4a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nvtabular.workflow import Workflow\n",
    "workflow_stored_path = os.path.join(input_path, \"workflow\")\n",
    "\n",
    "workflow = Workflow.load(workflow_stored_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6e7725-29e5-4509-9767-265a4cc19159",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddd04fc0-af4f-4139-8ac9-06cd97151eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = workflow.remove_inputs([\"rating\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143795f2",
   "metadata": {},
   "source": [
    "## Load Model\n",
    "\n",
    "After loading the workflow, we will load the model. This model was trained with the output of the workflow [merlin models alibaba dataset]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8da5e606",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-27 03:26:17.198959: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-03-27 03:26:19.335383: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21040 MB memory:  -> device: 0, name: Quadro RTX 8000, pci bus id: 0000:15:00.0, compute capability: 7.5\n",
      "2022-03-27 03:26:19.336065: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 46062 MB memory:  -> device: 1, name: Quadro RTX 8000, pci bus id: 0000:2d:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf_model_path = os.path.join(input_path, \"model\")\n",
    "\n",
    "model = tf.keras.models.load_model(tf_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53908458",
   "metadata": {},
   "source": [
    "## Create the Ensemble Graph\n",
    "\n",
    "Once we have both the model and the workflow loaded we can start creating the Ensemble Graph. This graph is created by the user, the goal is to illustrate the path of data through your full system. In this example we will only be serving a workflow with a model, but you can add other components that might be necessary to comply with business logic requirements.\n",
    "\n",
    "For this example, because we have two components a model and a workflow we will require two operators. These Ensemble operators, also known as Inference Operators, are meant to abstract away all the \"hard parts\" of loading a specific component (i.e. workflow or model) into tritonserver. \n",
    "\n",
    "In the following code block we will leverage two Inference operators, the TransformWorkflow operator and the PredictTensorflow operator. The TransformWorkflow operator will ensure the workflow is correctly saved and packaged with reqruired config, so that tritonserver will know how to load it. The PredictTensorflow operator will do something similar with the model we loaded before. \n",
    "\n",
    "Lets give it a try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f80e5cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from merlin.systems.dag.ops.workflow import TransformWorkflow\n",
    "from merlin.systems.dag.ops.tensorflow import PredictTensorflow\n",
    "\n",
    "triton_chain = workflow.input_schema.column_names >> TransformWorkflow(workflow) >> PredictTensorflow(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b916afa",
   "metadata": {},
   "source": [
    "## Export Graph as Ensemble\n",
    "\n",
    "The last step is to create the ensemble artifacts that tritonserver can consume. To make these artifacts we need to import the Ensemble class. It is responsible with interpreting the graph and exporting the correct files for tritonserver.\n",
    "\n",
    "To create an Ensemble object, the class consume a graph. Now the ensemble object is bonded to the supplied graph. The next step is to export the graph artifacts.\n",
    "\n",
    "When you are exporting an ensemble graph you must supply two objects, the path to export the graph and a schema representing the starting input of the graph. In otherwords, the input to the graph is are the inputs to the first operator of your graph. Usually this is a single column, user_id. \n",
    "Below you will see that we create a ColumnSchema for the expected \"user_id\" input, which is turned into a Schema. \n",
    "\n",
    "Once we have the graph, the export path and the request schema, we can create the Ensemble and export it. \n",
    "\n",
    "Lets take a look below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c9a7cd2-e14e-4b37-b0af-3dc3931c9ff9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'userId', 'tags': set(), 'properties': {}, 'dtype': dtype('int64'), 'is_list': False, 'is_ragged': False}, {'name': 'movieId', 'tags': set(), 'properties': {}, 'dtype': dtype('int64'), 'is_list': False, 'is_ragged': False}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow.input_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c102204b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) args_0 with unsupported characters which will be renamed to args_0_1 in the SavedModel.\n",
      "2022-03-27 03:26:21.025378: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, rating_binary/binary_classification_task/output_layer_layer_call_fn while saving (showing 5 of 48). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /merlin-models-data/movielens/ensemble/1_predicttensorflow/1/model.savedmodel/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /merlin-models-data/movielens/ensemble/1_predicttensorflow/1/model.savedmodel/assets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(name: \"ensemble_model\"\n",
       " platform: \"ensemble\"\n",
       " input {\n",
       "   name: \"userId\"\n",
       "   data_type: TYPE_INT64\n",
       "   dims: -1\n",
       "   dims: -1\n",
       " }\n",
       " input {\n",
       "   name: \"movieId\"\n",
       "   data_type: TYPE_INT64\n",
       "   dims: -1\n",
       "   dims: -1\n",
       " }\n",
       " output {\n",
       "   name: \"output_1\"\n",
       "   data_type: TYPE_FP32\n",
       "   dims: -1\n",
       "   dims: -1\n",
       " }\n",
       " ensemble_scheduling {\n",
       "   step {\n",
       "     model_name: \"0_transformworkflow\"\n",
       "     model_version: -1\n",
       "     input_map {\n",
       "       key: \"movieId\"\n",
       "       value: \"movieId\"\n",
       "     }\n",
       "     input_map {\n",
       "       key: \"userId\"\n",
       "       value: \"userId\"\n",
       "     }\n",
       "     output_map {\n",
       "       key: \"movie_id\"\n",
       "       value: \"movie_id_0\"\n",
       "     }\n",
       "     output_map {\n",
       "       key: \"user_id\"\n",
       "       value: \"user_id_0\"\n",
       "     }\n",
       "   }\n",
       "   step {\n",
       "     model_name: \"1_predicttensorflow\"\n",
       "     model_version: -1\n",
       "     input_map {\n",
       "       key: \"movie_id\"\n",
       "       value: \"movie_id_0\"\n",
       "     }\n",
       "     input_map {\n",
       "       key: \"user_id\"\n",
       "       value: \"user_id_0\"\n",
       "     }\n",
       "     output_map {\n",
       "       key: \"output_1\"\n",
       "       value: \"output_1\"\n",
       "     }\n",
       "   }\n",
       " },\n",
       " [name: \"0_transformworkflow\"\n",
       "  input {\n",
       "    name: \"userId\"\n",
       "    data_type: TYPE_INT64\n",
       "    dims: -1\n",
       "    dims: 1\n",
       "  }\n",
       "  input {\n",
       "    name: \"movieId\"\n",
       "    data_type: TYPE_INT64\n",
       "    dims: -1\n",
       "    dims: 1\n",
       "  }\n",
       "  output {\n",
       "    name: \"movie_id\"\n",
       "    data_type: TYPE_INT32\n",
       "    dims: -1\n",
       "    dims: 1\n",
       "  }\n",
       "  output {\n",
       "    name: \"user_id\"\n",
       "    data_type: TYPE_INT32\n",
       "    dims: -1\n",
       "    dims: 1\n",
       "  }\n",
       "  parameters {\n",
       "    key: \"cats\"\n",
       "    value {\n",
       "    }\n",
       "  }\n",
       "  parameters {\n",
       "    key: \"conts\"\n",
       "    value {\n",
       "    }\n",
       "  }\n",
       "  parameters {\n",
       "    key: \"output_model\"\n",
       "    value {\n",
       "    }\n",
       "  }\n",
       "  parameters {\n",
       "    key: \"python_module\"\n",
       "    value {\n",
       "      string_value: \"merlin.systems.triton.workflow_model\"\n",
       "    }\n",
       "  }\n",
       "  backend: \"nvtabular\",\n",
       "  name: \"1_predicttensorflow\"\n",
       "  platform: \"tensorflow_savedmodel\"\n",
       "  input {\n",
       "    name: \"movie_id\"\n",
       "    data_type: TYPE_INT32\n",
       "    dims: -1\n",
       "    dims: 1\n",
       "  }\n",
       "  input {\n",
       "    name: \"user_id\"\n",
       "    data_type: TYPE_INT32\n",
       "    dims: -1\n",
       "    dims: 1\n",
       "  }\n",
       "  output {\n",
       "    name: \"output_1\"\n",
       "    data_type: TYPE_FP32\n",
       "    dims: -1\n",
       "    dims: 1\n",
       "  }\n",
       "  parameters {\n",
       "    key: \"TF_GRAPH_TAG\"\n",
       "    value {\n",
       "      string_value: \"serve\"\n",
       "    }\n",
       "  }\n",
       "  parameters {\n",
       "    key: \"TF_SIGNATURE_DEF\"\n",
       "    value {\n",
       "      string_value: \"serving_default\"\n",
       "    }\n",
       "  }\n",
       "  backend: \"tensorflow\"])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from merlin.systems.dag.ensemble import Ensemble\n",
    "from merlin.schema import Schema, ColumnSchema\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "ensemble = Ensemble(triton_chain, workflow.input_schema)\n",
    "\n",
    "\n",
    "export_path = os.path.join(input_path, \"ensemble\")\n",
    "# request_schema = Schema(\n",
    "#     [\n",
    "#         ColumnSchema(\"user_id\", dtype=np.int64),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "ensemble.export(export_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4a27cc-f23a-4cec-9318-c44e0d344cc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "96aa55fb",
   "metadata": {},
   "source": [
    "## Verification of Ensemble Artifacts\n",
    "\n",
    "Once the ensemble export has completed successfully, we can check the export path for the aforementioned graph artifacts. You should see a file structure that represents a ordering number followed by an operator identifier(i.e. 1_transformworkflow, 2_predicttensorflow). \n",
    "\n",
    "Inside each of those folders, there should be a config.pbtxt and a folder with a number, representing a version, usually 1. The artifacts for the given operator are found inside the version folder. These artifacts vary depending on the operator in question. \n",
    "\n",
    "Please see the snapshot below for verification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b99146c",
   "metadata": {},
   "source": [
    "## Starting Triton Server\n",
    "\n",
    "After we have exported the ensemble, we are ready to start the triton server. First ensure it is installed in your environment otherwise find more install information here[link to triton build and install documentation]. Once installation is verified, you can start triton server by using the following command:\n",
    "\n",
    "`tritonserver --model-repository=/ensemble_export_path/`\n",
    "\n",
    "The export path should be the same as used above duirng ensemble export."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75e2eb9",
   "metadata": {},
   "source": [
    "## Sending a request to Triton\n",
    "\n",
    "Now that our tritonserver instance is running, we can send a request to it. This request will be composed of values that correspond to the request schema created when exporting the ensemble graph.\n",
    "\n",
    "In this case, the request will only have one item, user id. In the code below we will create a request to send to triton and send it. We will then analyze the response, to show the full end to end experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21a693ab-0ee3-443e-8ad4-fad030ec1985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tritonhttpclient/__init__.py:31: DeprecationWarning: The package `tritonhttpclient` is deprecated and will be removed in a future version. Please use instead `tritonclient.http`\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import tritonhttpclient\n",
    "\n",
    "# create triton client\n",
    "try:\n",
    "    triton_client = tritonhttpclient.InferenceServerClient(url=\"localhost:8000\", verbose=True)\n",
    "    print(\"client created.\")\n",
    "except Exception as e:\n",
    "    print(\"channel creation failed: \" + str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae48a6ee-d585-421b-bb5a-1a5c2edca058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET /v2/health/live, headers None\n"
     ]
    },
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/geventhttpclient/connectionpool.py:163\u001b[0m, in \u001b[0;36mConnectionPool.get_socket\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_socket_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m gevent\u001b[38;5;241m.\u001b[39mqueue\u001b[38;5;241m.\u001b[39mEmpty:\n",
      "File \u001b[0;32msrc/gevent/queue.py:335\u001b[0m, in \u001b[0;36mgevent._gevent_cqueue.Queue.get\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/gevent/queue.py:350\u001b[0m, in \u001b[0;36mgevent._gevent_cqueue.Queue.get\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/gevent/queue.py:319\u001b[0m, in \u001b[0;36mgevent._gevent_cqueue.Queue._Queue__get_or_peek\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mEmpty\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# ensure triton is in a good state\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtriton_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_server_live\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m triton_client\u001b[38;5;241m.\u001b[39mget_model_repository_index()\n\u001b[1;32m      4\u001b[0m triton_client\u001b[38;5;241m.\u001b[39mload_model(model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensemble_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tritonclient/http/__init__.py:341\u001b[0m, in \u001b[0;36mInferenceServerClient.is_server_live\u001b[0;34m(self, headers, query_params)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;124;03m\"\"\"Contact the inference server and get liveness.\u001b[39;00m\n\u001b[1;32m    318\u001b[0m \n\u001b[1;32m    319\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    337\u001b[0m \n\u001b[1;32m    338\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    340\u001b[0m request_uri \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mv2/health/live\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 341\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest_uri\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_uri\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mquery_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tritonclient/http/__init__.py:266\u001b[0m, in \u001b[0;36mInferenceServerClient._get\u001b[0;34m(self, request_uri, headers, query_params)\u001b[0m\n\u001b[1;32m    264\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client_stub\u001b[38;5;241m.\u001b[39mget(request_uri, headers\u001b[38;5;241m=\u001b[39mheaders)\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 266\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client_stub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest_uri\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_verbose:\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28mprint\u001b[39m(response)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/geventhttpclient/client.py:266\u001b[0m, in \u001b[0;36mHTTPClient.get\u001b[0;34m(self, request_uri, headers)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, request_uri, headers\u001b[38;5;241m=\u001b[39m{}):\n\u001b[0;32m--> 266\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMETHOD_GET\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_uri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/geventhttpclient/client.py:226\u001b[0m, in \u001b[0;36mHTTPClient.request\u001b[0;34m(self, method, request_uri, body, headers)\u001b[0m\n\u001b[1;32m    223\u001b[0m attempts_left \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection_pool\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 226\u001b[0m     sock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_socket\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    228\u001b[0m         _request \u001b[38;5;241m=\u001b[39m request\u001b[38;5;241m.\u001b[39mencode()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/geventhttpclient/connectionpool.py:166\u001b[0m, in \u001b[0;36mConnectionPool.get_socket\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m gevent\u001b[38;5;241m.\u001b[39mqueue\u001b[38;5;241m.\u001b[39mEmpty:\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_socket\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m    168\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_semaphore\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/geventhttpclient/connectionpool.py:127\u001b[0m, in \u001b[0;36mConnectionPool._create_socket\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_error:\n\u001b[0;32m--> 127\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m first_error\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    130\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot resolve \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_host, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_port))\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/geventhttpclient/connectionpool.py:114\u001b[0m, in \u001b[0;36mConnectionPool._create_socket\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    113\u001b[0m     sock\u001b[38;5;241m.\u001b[39msettimeout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnection_timeout)\n\u001b[0;32m--> 114\u001b[0m     sock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connect_socket\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msock_info\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter_connect(sock)\n\u001b[1;32m    116\u001b[0m     sock\u001b[38;5;241m.\u001b[39msettimeout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork_timeout)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/geventhttpclient/connectionpool.py:136\u001b[0m, in \u001b[0;36mConnectionPool._connect_socket\u001b[0;34m(self, sock, address)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_connect_socket\u001b[39m(\u001b[38;5;28mself\u001b[39m, sock, address):\n\u001b[0;32m--> 136\u001b[0m     \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43maddress\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setup_proxy(sock)\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sock\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/gevent/_socketcommon.py:607\u001b[0m, in \u001b[0;36mSocketMixin.connect\u001b[0;34m(self, address)\u001b[0m\n\u001b[1;32m    605\u001b[0m err \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgetsockopt(__socket__\u001b[38;5;241m.\u001b[39mSOL_SOCKET, __socket__\u001b[38;5;241m.\u001b[39mSO_ERROR)\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m err:\n\u001b[0;32m--> 607\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _SocketError(err, strerror(err))\n\u001b[1;32m    608\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mconnect_ex(address)\n\u001b[1;32m    610\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result \u001b[38;5;129;01mor\u001b[39;00m result \u001b[38;5;241m==\u001b[39m EISCONN:\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused"
     ]
    }
   ],
   "source": [
    "# ensure triton is in a good state\n",
    "triton_client.is_server_live()\n",
    "triton_client.get_model_repository_index()\n",
    "triton_client.load_model(model_name=\"ensemble_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b78283b-d59e-4431-b1b1-089430aff99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data for request\n",
    "batch = df_lib.read_parquet(\n",
    "    os.path.join(input_path, \"valid.parquet\"), num_rows=3, columns=workflow.input_schema.column_names\n",
    ")\n",
    "print(batch)\n",
    "\n",
    "\n",
    "# create inputs and outputs\n",
    "inputs = nvt_triton.convert_df_to_triton_input([\"user_id\"], batch, grpcclient.InferInput)\n",
    "\n",
    "outputs = [\n",
    "    grpcclient.InferRequestedOutput(col)\n",
    "    for col in [\"user_id\", \"movieId\", \"genres__nnzs\", \"genres__values\"]\n",
    "]\n",
    "\n",
    "# send request to tritonserver\n",
    "with grpcclient.InferenceServerClient(\"localhost:8001\") as client:\n",
    "    response = client.infer(\"ensemble\", inputs, request_id=\"1\", outputs=outputs)\n",
    "\n",
    "\n",
    "# access individual response columns to get values back.\n",
    "for col in [\"userId\", \"movieId\", \"genres__nnzs\", \"genres__values\"]:\n",
    "    print(col, response.as_numpy(col), response.as_numpy(col).shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
