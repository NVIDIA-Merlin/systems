{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "620dd990",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Copyright 2022 NVIDIA Corporation. All Rights Reserved.\n",
                "#\n",
                "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
                "# you may not use this file except in compliance with the License.\n",
                "# You may obtain a copy of the License at\n",
                "#\n",
                "#     http://www.apache.org/licenses/LICENSE-2.0\n",
                "#\n",
                "# Unless required by applicable law or agreed to in writing, software\n",
                "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
                "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
                "# See the License for the specific language governing permissions and\n",
                "# limitations under the License.\n",
                "# =============================================================================="
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f550a0e5",
            "metadata": {},
            "source": [
                "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
                "\n",
                "# Getting Started with Merlin Systems \n",
                "\n",
                "## Overview\n",
                "\n",
                "NVIDIA Merlin is an open source framework that accelerates and scales end-to-end recommender system pipelines on GPU. The Merlin framework is broken up into several sub components, these include: merlin-core, merlin-models, nvtabular and merlin-systems. Merlin Systems will be the focus of this example.\n",
                "\n",
                "The purpose of the Merlin Systems library is to make it easy for merlin users to quickly deploy their recommender systems from development to triton. We extended the same user-friendly API users are accustomed to in NVTabular and leveraged it to accommodate deploying your recommender system components to triton. \n",
                "\n",
                "There are some things we need ensure before we continue with this Notebook. Please ensure you have a working workflow and model stored in an accessible location. As previously mentioned, Merlin Systems will take the data preprocessing workflow defined in nvtabular and load that into triton as a model. Subsequently it will do the same for the trained model. Lets take a closer look in the rest of this notebook at how merlin systems makes deploying to tritonserver simple and effortless.\n",
                "\n",
                "\n",
                "**Be sure to check the other components of the Merlin framework, they can help you **\n",
                "\n",
                "### Learning objectives\n",
                "\n",
                "In this notebook, we learn how to deploy a NVTabular Workflow and a trained model from Merlin Models to tritonserver.\n",
                "- Load NVtabular Workflow\n",
                "- Load Pre-trained Merlin Models model\n",
                "- Create Ensemble Graph\n",
                "- Export Ensemble Graph\n",
                "- Run Tritonserver\n",
                "- Send Request to Tritonserver\n",
                "\n",
                "### Dataset\n",
                "\n",
                "In this notebook, we will be leveraging the Alibaba dataset[add link here]. It is important to note that the steps will take in this notebook are generalized and can be applied to any set of workflow and models. To see how the data is transformed please check the NVTabular [lin here nvtabular example] example for the Alibaba dataset. And to see how an Alibaba dataset trained model is created check the merlin-models project[link here merlin-models]\n",
                "\n",
                "### Tools\n",
                "\n",
                "- NVTabular\n",
                "- Merlin Models\n",
                "- Merlin Systems"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "222fde5f",
            "metadata": {},
            "source": [
                "## Load Workflow\n",
                "\n",
                "First, we will load the workflow created in with this example [link to nvtabular workflow example for alibaba dataset]. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b1dc4a71",
            "metadata": {},
            "outputs": [],
            "source": [
                "from nvtabular.workflow import Workflow\n",
                "workflow_stored_path = \"./workflow/\"\n",
                "\n",
                "workflow = Workflow.load(workflow_stored_path)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "143795f2",
            "metadata": {},
            "source": [
                "## Load Model\n",
                "\n",
                "After loading the workflow, we will load the model. This model was trained with the output of the workflow [merlin models alibaba dataset]."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8da5e606",
            "metadata": {},
            "outputs": [],
            "source": [
                "import tensorflow as tf\n",
                "tf_model_path = \"./model/\"\n",
                "\n",
                "model = tf.keras.models.load_model(tf_model_path)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "53908458",
            "metadata": {},
            "source": [
                "## Create the Ensemble Graph\n",
                "\n",
                "Once we have both the model and the workflow loaded we can start creating the Ensemble Graph. This graph is created by the user, the goal is to illustrate the path of data through your full system. In this example we will only be serving a workflow with a model, but you can add other components that might be necessary to comply with business logic requirements.\n",
                "\n",
                "For this example, because we have two components a model and a workflow we will require two operators. These Ensemble operators, also known as Inference Operators, are meant to abstract away all the \"hard parts\" of loading a specific component (i.e. workflow or model) into tritonserver. \n",
                "\n",
                "In the following code block we will leverage two Inference operators, the TransformWorkflow operator and the PredictTensorflow operator. The TransformWorkflow operator will ensure the workflow is correctly saved and packaged with reqruired config, so that tritonserver will know how to load it. The PredictTensorflow operator will do something similar with the model we loaded before. \n",
                "\n",
                "Lets give it a try."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "f80e5cc8",
            "metadata": {},
            "outputs": [],
            "source": [
                "from merlin.systems.dag.ops.workflow import TransformWorkflow\n",
                "from merlin.systems.dag.ops.tensorflow import PredictTensorflow\n",
                "\n",
                "triton_chain = TransformWorkflow(workflow) >> PredictTensorflow(model)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "8b916afa",
            "metadata": {},
            "source": [
                "## Export Graph as Ensemble\n",
                "\n",
                "The last step is to create the ensemble artifacts that tritonserver can consume. To make these artifacts we need to import the Ensemble class. It is responsible with interpreting the graph and exporting the correct files for tritonserver.\n",
                "\n",
                "To create an Ensemble object, the class consume a graph. Now the ensemble object is bonded to the supplied graph. The next step is to export the graph artifacts.\n",
                "\n",
                "When you are exporting an ensemble graph you must supply two objects, the path to export the graph and a schema representing the starting input of the graph. In otherwords, the input to the graph is are the inputs to the first operator of your graph. Usually this is a single column, user_id. \n",
                "Below you will see that we create a ColumnSchema for the expected \"user_id\" input, which is turned into a Schema. \n",
                "\n",
                "Once we have the graph, the export path and the request schema, we can create the Ensemble and export it. \n",
                "\n",
                "Lets take a look below."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "c102204b",
            "metadata": {},
            "outputs": [],
            "source": [
                "from merlin.systems.dag.ensemble import Ensemble\n",
                "from merlin.schema import Schema, ColumnSchema\n",
                "import numpy as np\n",
                "\n",
                "\n",
                "ensemble = Ensemble(triton_chain)\n",
                "\n",
                "\n",
                "export_path = \"./export_ensemble/\"\n",
                "request_schema = Schema(\n",
                "    [\n",
                "        ColumnSchema(\"user_id\", dtype=np.int64),\n",
                "    ]\n",
                ")\n",
                "\n",
                "ensemble.export(export_path, request_schema)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "96aa55fb",
            "metadata": {},
            "source": [
                "## Verification of Ensemble Artifacts\n",
                "\n",
                "Once the ensemble export has completed successfully, we can check the export path for the aforementioned graph artifacts. You should see a file structure that represents a ordering number followed by an operator identifier(i.e. 1_transformworkflow, 2_predicttensorflow). \n",
                "\n",
                "Inside each of those folders, there should be a config.pbtxt and a folder with a number, representing a version, usually 1. The artifacts for the given operator are found inside the version folder. These artifacts vary depending on the operator in question. \n",
                "\n",
                "Please see the snapshot below for verification."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "0b99146c",
            "metadata": {},
            "source": [
                "## Starting Triton Server\n",
                "\n",
                "After we have exported the ensemble, we are ready to start the triton server. First ensure it is installed in your environment otherwise find more install information here[link to triton build and install documentation]. Once installation is verified, you can start triton server by using the following command:\n",
                "\n",
                "`tritonserver --model-repository=/ensemble_export_path/`\n",
                "\n",
                "The export path should be the same as used above duirng ensemble export."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c75e2eb9",
            "metadata": {},
            "source": [
                "## Sending a request to Triton\n",
                "\n",
                "Now that our tritonserver instance is running, we can send a request to it. This request will be composed of values that correspond to the request schema created when exporting the ensemble graph.\n",
                "\n",
                "In this case, the request will only have one item, user id. In the code below we will create a request to send to triton and send it. We will then analyze the response, to show the full end to end experience."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "0357c655",
            "metadata": {},
            "outputs": [],
            "source": [
                "import tritonhttpclient\n",
                "\n",
                "# create triton client\n",
                "try:\n",
                "    triton_client = tritonhttpclient.InferenceServerClient(url=\"localhost:8000\", verbose=True)\n",
                "    print(\"client created.\")\n",
                "except Exception as e:\n",
                "    print(\"channel creation failed: \" + str(e))\n",
                "\n",
                "\n",
                "# ensure triton is in a good state\n",
                "triton_client.is_server_live()\n",
                "triton_client.get_model_repository_index()\n",
                "triton_client.load_model(model_name=\"ensemble\")\n",
                "\n",
                "# read in data for request\n",
                "batch = df_lib.read_parquet(\n",
                "    os.path.join(INPUT_DATA_DIR, \"valid.parquet\"), num_rows=3, columns=[\"user_id\"]\n",
                ")\n",
                "print(batch)\n",
                "\n",
                "\n",
                "# create inputs and outputs\n",
                "inputs = nvt_triton.convert_df_to_triton_input([\"user_id\"], batch, grpcclient.InferInput)\n",
                "\n",
                "outputs = [\n",
                "    grpcclient.InferRequestedOutput(col)\n",
                "    for col in [\"user_id\", \"movieId\", \"genres__nnzs\", \"genres__values\"]\n",
                "]\n",
                "\n",
                "# send request to tritonserver\n",
                "with grpcclient.InferenceServerClient(\"localhost:8001\") as client:\n",
                "    response = client.infer(\"ensemble\", inputs, request_id=\"1\", outputs=outputs)\n",
                "\n",
                "\n",
                "# access individual response columns to get values back.\n",
                "for col in [\"userId\", \"movieId\", \"genres__nnzs\", \"genres__values\"]:\n",
                "    print(col, response.as_numpy(col), response.as_numpy(col).shape)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
