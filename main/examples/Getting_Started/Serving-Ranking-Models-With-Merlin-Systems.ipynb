{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "620dd990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2022 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f550a0e5",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# Serving Ranking Models With Merlin Systems\n",
    "\n",
    "## Overview\n",
    "\n",
    "NVIDIA Merlin is an open source framework that accelerates and scales end-to-end recommender system pipelines. The Merlin framework is broken up into several sub components, these include: Merlin-Core, Merlin-Models, NVTabular and Merlin-Systems. Merlin Systems will be the focus of this example.\n",
    "\n",
    "The purpose of the Merlin Systems library is to make it easy for Merlin users to quickly deploy their recommender systems from development to [Triton Inference Server](https://github.com/triton-inference-server/server). We extended the same user-friendly API users are accustomed to in NVTabular and leveraged it to accommodate deploying your recommender system components to Triton. \n",
    "\n",
    "There are some things we need ensure before we continue with this Notebook. Please ensure you have a working workflow and model stored in an accessible location. As previously mentioned, Merlin Systems will take the data preprocessing workflow defined in NVTabular and load that into Triton as a model. Subsequently it will do the same for the trained model. Lets take a closer look in the rest of this notebook at how Merlin systems makes deploying to Triton simple and effortless.\n",
    "\n",
    "\n",
    "**Be sure to check the other components of the Merlin framework, they can help you **\n",
    "\n",
    "### Learning objectives\n",
    "\n",
    "In this notebook, we learn how to deploy a NVTabular Workflow and a trained Tensorflow model from Merlin Models to Triton.\n",
    "- Load NVTabular Workflow\n",
    "- Load Pre-trained Merlin Models model\n",
    "- Create Ensemble Graph\n",
    "- Export Ensemble Graph\n",
    "- Run Tritonserver\n",
    "- Send Request to Tritonserver\n",
    "\n",
    "### Dataset\n",
    "\n",
    "In this notebook, we will be leveraging the [Alibaba dataset](https://tianchi.aliyun.com/dataset/dataDetail?dataId=408#1). It is important to note that the steps will take in this notebook are generalized and can be applied to any set of workflow and models. To see how the data is transformed please check the [NVTabular](https://github.com/NVIDIA-Merlin/NVTabular) example for the Alibaba dataset. And to see how an Alibaba dataset trained model is created check the [merlin-models](https://github.com/NVIDIA-Merlin/models)\n",
    "\n",
    "### Tools\n",
    "\n",
    "- NVTabular\n",
    "- Merlin Models\n",
    "- Merlin Systems\n",
    "- Triton Inference Server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222fde5f",
   "metadata": {},
   "source": [
    "## Load an NVTabular Workflow\n",
    "\n",
    "First, we load the `nvtabular.Workflow` that we created in with this [example](https://github.com/NVIDIA-Merlin/models/blob/main/examples/04-Exporting-ranking-models.ipynb). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1dc4a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/workshared/nvtabular/nvtabular/workflow/workflow.py:373: UserWarning: Loading workflow generated with cudf version 21.10.00a+345.ge05bd4bf3c.dirty - but we are running cudf 21.12.02. This might cause issues\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TF_GPU_ALLOCATOR\"]=\"cuda_malloc_async\"\n",
    "from nvtabular.workflow import Workflow\n",
    "\n",
    "input_path = os.environ.get(\"INPUT_FOLDER\", \"/workspace/data/\")\n",
    "\n",
    "workflow_stored_path = os.path.join(input_path, \"workflow\")\n",
    "\n",
    "workflow = Workflow.load(workflow_stored_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746fba56-36af-44a3-8beb-34ca026648be",
   "metadata": {},
   "source": [
    "After we load the workflow, we remove the label columns from it's inputs. This removes all columns with the `TARGET` tag from the workflow. We do this because we need to set the workflow to only  require the features needed to predict, not train, when creating an inference pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcef202e-fa6f-4c23-aca6-0b965faf637e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<nvtabular.workflow.workflow.Workflow at 0x7fa3240d10a0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from merlin.schema.tags import Tags\n",
    "\n",
    "label_columns = workflow.output_schema.select_by_tag(Tags.TARGET).column_names\n",
    "workflow.remove_inputs(label_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143795f2",
   "metadata": {},
   "source": [
    "## Load a Tensorflow Model\n",
    "\n",
    "After loading the workflow, we load the model. This model was trained with the output of the workflow from this [example](https://github.com/NVIDIA-Merlin/models/blob/main/examples/04-Exporting-ranking-models.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8da5e606",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-07 18:59:24.694276: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-04-07 18:59:26.773289: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:214] Using CUDA malloc Async allocator for GPU: 0\n",
      "2022-04-07 18:59:26.773451: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18864 MB memory:  -> device: 0, name: Quadro RTX 8000, pci bus id: 0000:15:00.0, compute capability: 7.5\n",
      "2022-04-07 18:59:26.774073: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:214] Using CUDA malloc Async allocator for GPU: 1\n",
      "2022-04-07 18:59:26.774136: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 42858 MB memory:  -> device: 1, name: Quadro RTX 8000, pci bus id: 0000:2d:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf_model_path = os.path.join(input_path, \"dlrm\")\n",
    "\n",
    "model = tf.keras.models.load_model(tf_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53908458",
   "metadata": {},
   "source": [
    "## Create the Ensemble Graph\n",
    "\n",
    "Once we have both the model and the workflow loaded we can create the Ensemble Graph. This graph is created by the user, the goal is to illustrate the path of data through your full system. In this example we only serve a workflow with a model, but you can add other components that might be necessary to comply with business logic requirements.\n",
    "\n",
    "For this example, because we have two components a model and a workflow we will require two operators. These operators, also known as Inference Operators, are meant to abstract away all the \"hard parts\" of loading a specific component (i.e. workflow or model) into tritonserver. \n",
    "\n",
    "In the following code block we will leverage two Inference operators, the TransformWorkflow operator and the PredictTensorflow operator. The TransformWorkflow operator ensures the workflow is correctly saved and packaged with the required config, so tritonserver will know how to load it. The PredictTensorflow operator will do something similar with the model, loaded before. \n",
    "\n",
    "Lets give it a try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f80e5cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from merlin.systems.dag.ops.workflow import TransformWorkflow\n",
    "from merlin.systems.dag.ops.tensorflow import PredictTensorflow\n",
    "\n",
    "serving_operators = workflow.input_schema.column_names >> TransformWorkflow(workflow) >> PredictTensorflow(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b916afa",
   "metadata": {},
   "source": [
    "## Export Graph as Ensemble\n",
    "\n",
    "The last step is to create the ensemble artifacts that tritonserver can consume. To make these artifacts import the `Ensemble` class. It is responsible with interpreting the graph and exporting the correct files for tritonserver.\n",
    "\n",
    "Below you will see that we create a ColumnSchema for the expected inputs to the workflow, which is a Schema. \n",
    "\n",
    "When you are creating an `Ensemble` object you supply the graph and a schema representing the starting input of the graph. the inputs to the ensemble graph are the inputs to the first operator of your graph. \n",
    "\n",
    "After you have created the `Ensemble` you export the graph, supplying an export path for the `Ensemble.export` function.\n",
    "\n",
    "This returns an ensemble config which represents the entire inference pipeline and a list of node-specific configs.\n",
    "\n",
    "Lets take a look below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c9a7cd2-e14e-4b37-b0af-3dc3931c9ff9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'user_id', 'tags': set(), 'properties': {}, 'dtype': dtype('O'), 'is_list': False, 'is_ragged': False}, {'name': 'item_id', 'tags': set(), 'properties': {}, 'dtype': dtype('O'), 'is_list': False, 'is_ragged': False}, {'name': 'item_category', 'tags': set(), 'properties': {}, 'dtype': dtype('O'), 'is_list': False, 'is_ragged': False}, {'name': 'item_shop', 'tags': set(), 'properties': {}, 'dtype': dtype('O'), 'is_list': False, 'is_ragged': False}, {'name': 'item_brand', 'tags': set(), 'properties': {}, 'dtype': dtype('O'), 'is_list': False, 'is_ragged': False}, {'name': 'user_shops', 'tags': set(), 'properties': {}, 'dtype': dtype('O'), 'is_list': False, 'is_ragged': False}, {'name': 'user_profile', 'tags': set(), 'properties': {}, 'dtype': dtype('O'), 'is_list': False, 'is_ragged': False}, {'name': 'user_group', 'tags': set(), 'properties': {}, 'dtype': dtype('O'), 'is_list': False, 'is_ragged': False}, {'name': 'user_gender', 'tags': set(), 'properties': {}, 'dtype': dtype('O'), 'is_list': False, 'is_ragged': False}, {'name': 'user_age', 'tags': set(), 'properties': {}, 'dtype': dtype('O'), 'is_list': False, 'is_ragged': False}, {'name': 'user_consumption_2', 'tags': set(), 'properties': {}, 'dtype': dtype('O'), 'is_list': False, 'is_ragged': False}, {'name': 'user_is_occupied', 'tags': set(), 'properties': {}, 'dtype': dtype('O'), 'is_list': False, 'is_ragged': False}, {'name': 'user_geography', 'tags': set(), 'properties': {}, 'dtype': dtype('O'), 'is_list': False, 'is_ragged': False}, {'name': 'user_intentions', 'tags': set(), 'properties': {}, 'dtype': dtype('O'), 'is_list': False, 'is_ragged': False}, {'name': 'user_brands', 'tags': set(), 'properties': {}, 'dtype': dtype('O'), 'is_list': False, 'is_ragged': False}, {'name': 'user_categories', 'tags': set(), 'properties': {}, 'dtype': dtype('O'), 'is_list': False, 'is_ragged': False}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow.input_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c102204b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-07 18:59:31.098398: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, click/binary_classification_task/output_layer_layer_call_fn while saving (showing 5 of 48). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /raid/data/ali/processed/ensemble/1_predicttensorflow/1/model.savedmodel/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /raid/data/ali/processed/ensemble/1_predicttensorflow/1/model.savedmodel/assets\n"
     ]
    }
   ],
   "source": [
    "from merlin.systems.dag.ensemble import Ensemble\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "ensemble = Ensemble(serving_operators, workflow.input_schema)\n",
    "\n",
    "export_path = os.path.join(input_path, \"ensemble\")\n",
    "\n",
    "ens_conf, node_confs = ensemble.export(export_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96aa55fb",
   "metadata": {},
   "source": [
    "## Verification of Ensemble Artifacts\n",
    "\n",
    "Once the ensemble export has completed successfully, we can check the export path for the graph's artifacts. You should see a file structure that represents a ordering number followed by an operator identifier(i.e. `1_transformworkflow`, `2_predicttensorflow`). \n",
    "\n",
    "Inside each of those directories, the `export` method writes a `config.pbtxt` file and a directory with a number. The number indicates the version and begins at 1. The artifacts for each operator are found inside the version folder. These artifacts vary depending on the operator in question. \n",
    "\n",
    "Please see the snapshot below for verification."
   ]
  },
  {
   "attachments": {
    "6f1f7c10-99d1-466e-9614-8ffa22f255ac.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "3cb65616-1d76-4232-91f4-c442297431f9",
   "metadata": {},
   "source": [
    "![image.png](attachment:6f1f7c10-99d1-466e-9614-8ffa22f255ac.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b99146c",
   "metadata": {},
   "source": [
    "## Starting Triton Server\n",
    "\n",
    "After we export the ensemble, we are ready to start the Triton Inference Server. First ensure it is installed in your environment otherwise find more install information here[link to triton build and install documentation]. Once installation is verified, you can start triton server by using the following command:\n",
    "\n",
    "`tritonserver --model-repository=/ensemble_export_path/ --backend-config=tensorflow,version=2`\n",
    "\n",
    "For the `--model-repository` argument, specify the same value as the `export_path` that you specified previously in the `ensemble.export` method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75e2eb9",
   "metadata": {},
   "source": [
    "## Retrieving Recommendations from Triton\n",
    "\n",
    "Now that our tritonserver instance is running, we can send a request to it. This request is composed of values that correspond to the request schema created when exporting the ensemble graph.\n",
    "\n",
    "We load original test data (`df_lib` is cudf if a GPU is available and pandas otherwise) as the `input_df` and we declare the `outputs` expected from the model. With both the `input_df` and `outputs` we can send an inference request to Triton, using `send_triton_request`. This inference response provided by Triton is parsed to return the expected predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e355213d-e452-4d43-8bba-d36b7f5694bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_1': array([[0.04845615],\n",
       "        [0.05534323],\n",
       "        [0.00968392]], dtype=float32)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from merlin.systems.triton.utils import send_triton_request\n",
    "from merlin.core.dispatch import get_lib\n",
    "\n",
    "df_lib = get_lib()\n",
    "original_data_path = os.environ.get(\"DATA_FOLDER\", \"/workspace/data/\")\n",
    "\n",
    "# read in data for request\n",
    "input_df = df_lib.read_parquet(\n",
    "    os.path.join(original_data_path,\"test\", \"test_0.parquet\"), num_rows=3, columns=workflow.input_schema.column_names\n",
    ")\n",
    "\n",
    "outputs = ensemble.graph.output_schema.column_names\n",
    "\n",
    "parsed_response = send_triton_request(input_df, outputs)\n",
    "parsed_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f29f71c-57c9-4c04-8657-daa16405a11b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
